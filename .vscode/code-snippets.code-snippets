{
	// Place your workshop-code workspace snippets here. Each snippet is defined under a snippet name and has a scope, prefix, body and 
	// description. Add comma separated ids of the languages where the snippet is applicable in the scope field. If scope 
	// is left empty or omitted, the snippet gets applied to all languages. The prefix is what is 
	// used to trigger the snippet and the body will be expanded and inserted. Possible variables are: 
	// $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders. 
	// Placeholders with the same ids are connected.
	// Example:
	// "Print to console": {
	// 	"scope": "javascript,typescript",
	// 	"prefix": "log",
	// 	"body": [
	// 		"console.log('$1');",
	// 		"$2"
	// 	],
	// 	"description": "Log output to console"
	// }

	"d1-start": {
		"prefix": "d1-start",
		"body": [
		  "import { ChatOpenAI } from \"@langchain/openai\"",
		  "import * as dotenv from \"dotenv\"",
		  "// 游릭 note the .env file -> https://platform.openai.com/api-keys",
		  "dotenv.config()",
		  "// 游릭 openAIApiKey, temperature ",
		  "let model = new ChatOpenAI()",
		  "let response = await model.invoke('What is the age of Bugs Bunny?')",
		  "console.log(response)",
		  "// 游릭 node --no-deprecation index.js"
		],
		"description": "A VS Code snippet for LangChain ChatOpenAI with dotenv and a predefined prompt."
	},

	"langchainPromptDemo": {
		"prefix": "d1-key-prompt",
		"body": [
		"// read prompt from keyboard",
		"// 游릭 npm i readline-promise",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"let prompt = await rl.question('How can I help you: ')",
		"// 游릭 short prompts only for demos + model: \"gpt-4\" ; why smaller is better for dev https://x.com/js_craft_hq/status/1872580473567994105",
		"let response = await model.invoke(prompt)",
		"console.log(response)",
		"// 游릭 Give me an easy trivia question from math",
		"rl.close()"
		],
		"description": "A VS Code snippet for a LangChain ChatOpenAI demo using readline-promise and dotenv."
	}, 
	

"d1-prompt-templates": {
    "prefix": "d1-prompt-templates",
    "body": [
      "// adding prompt templates",
      "import { ChatOpenAI } from \"@langchain/openai\"",
      "import * as dotenv from \"dotenv\"",
      "import * as readline from 'node:readline/promises'",
      "import { stdin as input, stdout as output } from 'node:process'",
      "import { PromptTemplate } from \"@langchain/core/prompts\"",
      "",
      "dotenv.config()",
      "let model = new ChatOpenAI()",
      "const prompt = new PromptTemplate({",
      "    inputVariables: [ \"level\", \"domain\"],",
      "    template: \"Give me a {level} trivia question from {domain}\"",
      "})",
      "",
      "let rl = readline.createInterface({ input, output })",
      "let level = await rl.question('游늵 Question level: ')",
      "let domain = await rl.question('游닀 Question domain: ')",
      "",
      "// 游릭 why is this async?",
      "const formattedPrompt = await prompt.format({level, domain})",
      "let response = await model.invoke(formattedPrompt)",
      "console.log(response)",
      "rl.close()",
      "// 游릭 see tokenUsage; 1 token is close to 1 word ",
    ],
    "description": "A VS Code snippet for LangChain ChatOpenAI with prompt templates, chains, and string output parser."
  }, 


"d1-chains-parsers": {
	"prefix": "d1-chains-parsers",
	"body": [
	"// adding chains and string output parser",
	"import { ChatOpenAI } from \"@langchain/openai\"",
	"import * as dotenv from \"dotenv\"",
	"import * as readline from 'node:readline/promises'",
	"import { stdin as input, stdout as output } from 'node:process'",
	"import { PromptTemplate } from \"@langchain/core/prompts\"",
	"import { StringOutputParser} from \"@langchain/core/output_parsers\"",
	"",
	"dotenv.config()",
	"let model = new ChatOpenAI()",
	"let rl = readline.createInterface({ input, output })",
	"",
	"const prompt = new PromptTemplate({",
	"    inputVariables: [ \"level\", \"domain\"],",
	"    template: \"Give me a {level} trivia question from {domain}\"",
	"})",
	"// 游릭 add ouput parser here / pipe LCEL",
	"const chain = prompt.pipe(model)",
	"",
	"let level = await rl.question('游늵 Question level: ')",
	"let domain = await rl.question('游닀 Question domain: ')",
	"",
	"let question = await chain.invoke({level, domain})",
	"console.log(question)",
	"",
	"rl.close()"
	],
	"description": "d1-chains-parsers"
},
  
"d1-mutiple-chains-comma-parser": {
	"prefix": "d1-mutiple-chains-comma-parser",
	"body": [
	"// mutiple chains and CommaSeparatedListOutputParser",
	"import { ChatOpenAI } from \"@langchain/openai\"",
	"import * as dotenv from \"dotenv\"",
	"import * as readline from 'node:readline/promises'",
	"import { stdin as input, stdout as output } from 'node:process'",
	"import { PromptTemplate } from \"@langchain/core/prompts\"",
	"import { StringOutputParser, CommaSeparatedListOutputParser} from \"@langchain/core/output_parsers\"",
	"",
	"dotenv.config()",
	"let model = new ChatOpenAI()",
	"let rl = readline.createInterface({ input, output })",
	"",
	"const qPrompt = new PromptTemplate({",
	"    inputVariables: [ \"level\", \"domain\"],",
	"    template: \"Give me a {level} trivia question from {domain}\"",
	"})",
	"",
	"const qChain = qPrompt.pipe(model).pipe(new StringOutputParser())",
	"",
	"let level = await rl.question('游늵 Question level: ')",
	"let domain = await rl.question('游닀 Question domain: ')",
	"// 游릭 chains - one output becomes the next input",
	"let question = await qChain.invoke({level, domain})",
	"console.log(question)",
	"",
	"const aPrompt = new PromptTemplate({",
	"    inputVariables: [ \"question\"],",
	"    template: \"Give 4 possible answers for {question}, separated by commas, 3 false and 1 correct, in a random order.\"",
	"})",
	"// 游릭 output parser here",
	"const aChain = aPrompt.pipe(model)",
	"",
	"let answers = await aChain.invoke({question})",
	"console.log(answers)",
	"// 游릭 we can use answers with a for each",
	"",
    "rl.close()"
	],
	"description": "d1-mutiple-chains-comma-parser"
},


"d1-structured-zod": {
	  "prefix": "d1-structured-zod",
	  "body": [
		"// StructuredOutputParser and ZOD",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"import { PromptTemplate } from \"@langchain/core/prompts\"",
		"import { StructuredOutputParser } from \"langchain/output_parsers\"",
		"import { z } from \"zod\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"// 游릭 in python this is Pydantic",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    {format_instructions}`",
		")",
		"",
		"const chain = prompt.pipe(model).pipe(parser)",
		"// 游릭 show this; parser.getFormatInstructions(); parsers are just part of the prompt",
		"let data = await chain.invoke({",
		"    format_instructions: parser.getFormatInstructions()",
		"})",
		"",
		"console.log(data)",
		"",
		"rl.close()"
	  ],
	  "description": "d1-structured-zod"
},


"d1-mem1-dowhile": {
	  "prefix": "d1-mem1-dowhile",
	  "body": [
		"// add memory part 1 + DO WHILE",
		"// 游릭 show GPT Conversation with the most expensive painting the the world ? + where it that?",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"// 游릭 ChatPromptTemplate vs PromptTemplate; why ?",
		"import { ChatPromptTemplate, PromptTemplate } from \"@langchain/core/prompts\"",
		"import { JsonOutputParser, StructuredOutputParser} from \"@langchain/core/output_parsers\"",
		"import { z } from \"zod\"",
		"import { MessagesPlaceholder } from \"@langchain/core/prompts\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"// 游릭 aks in prompt not to repeat the questions",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    Don't repeat previous questions \\n",
		"    {format_instructions}`",
		")",
		"",
		"const formattedPrompt = await prompt.format({",
		"    format_instructions: parser.getFormatInstructions()",
		"});",
		"",
		"const chatHistory = []",
		"",
		"// 游릭 a ChatPromptTemplate must have chat_history",
		"const chatPromptTemplate = ChatPromptTemplate.fromMessages([",
		"    new MessagesPlaceholder(\"chat_history\"),",
		"    [\"human\", \"{input}\"]",
		"])",
		"",
		"// 游릭 JsonOutputParser",
		"const chain = chatPromptTemplate.pipe(model).pipe(new JsonOutputParser())",
		"",
		"let oneMoreQuestion",
		"do {",
		"    const data = await chain.invoke({",
		"        input: formattedPrompt,",
		"        chat_history: chatHistory",
		"    })",
		"    console.log(data)",
		"    oneMoreQuestion = await rl.question('游눮 Ask one more question (y for yes):')",
		"} while(oneMoreQuestion == 'y')",
		"rl.close()"
	  ],
	  "description": "d1-mem1-dowhile"
},


"d1-mem2": {
	  "prefix": "d1-mem2",
	  "body": [
		"// keeping track of memory",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"import { ChatPromptTemplate, PromptTemplate } from \"@langchain/core/prompts\"",
		"import { JsonOutputParser, StructuredOutputParser} from \"@langchain/core/output_parsers\"",
		"import { z } from \"zod\"",
		"import { MessagesPlaceholder } from \"@langchain/core/prompts\"",
		"import { HumanMessage, AIMessage } from \"@langchain/core/messages\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    Don't repeat previous questions \\n",
		"    {format_instructions}`",
		")",
		"",
		"const formattedPrompt = await prompt.format({",
		"    format_instructions: parser.getFormatInstructions()",
		"});",
		"",
		"const chatHistory = []",
		"",
		"const chatPromptTemplate = ChatPromptTemplate.fromMessages([",
		"    new MessagesPlaceholder(\"chat_history\"),",
		"    [\"human\", \"{input}\"]",
		"])",
		"",
		"const chain = chatPromptTemplate.pipe(model).pipe(new JsonOutputParser())",
		"",
		"let oneMoreQuestion",
		"do {",
		"    const data = await chain.invoke({",
		"        input: formattedPrompt,",
		"        chat_history: chatHistory",
		"    })",
		"    console.log(data)",
		"// 游릭 HumanMessage and AIMessage",
		"    chatHistory.push(new HumanMessage(formattedPrompt))",
		"    chatHistory.push(new AIMessage(JSON.stringify(data)))",
		"    oneMoreQuestion = await rl.question('游눮 Ask one more question (y for yes):')",
		"} while(oneMoreQuestion == 'y')",
		"rl.close()"
	  ],
	  "description": "d1-mem2"
}
}