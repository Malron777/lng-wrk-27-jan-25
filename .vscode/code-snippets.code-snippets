{
	// Place your workshop-code workspace snippets here. Each snippet is defined under a snippet name and has a scope, prefix, body and 
	// description. Add comma separated ids of the languages where the snippet is applicable in the scope field. If scope 
	// is left empty or omitted, the snippet gets applied to all languages. The prefix is what is 
	// used to trigger the snippet and the body will be expanded and inserted. Possible variables are: 
	// $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders. 
	// Placeholders with the same ids are connected.
	// Example:
	// "Print to console": {
	// 	"scope": "javascript,typescript",
	// 	"prefix": "log",
	// 	"body": [
	// 		"console.log('$1');",
	// 		"$2"
	// 	],
	// 	"description": "Log output to console"
	// }

	"d1-start": {
		"prefix": "d1-start",
		"body": [
		  "import { ChatOpenAI } from \"@langchain/openai\"",
		  "import * as dotenv from \"dotenv\"",
		  "// üü¢ note the .env file -> https://platform.openai.com/api-keys",
		  "dotenv.config()",
		  "// üü¢ openAIApiKey, temperature ",
		  "let model = new ChatOpenAI()",
		  "let response = await model.invoke('What is the age of Bugs Bunny?')",
		  "console.log(response)",
		  "// üü¢ node --no-deprecation index.js"
		],
		"description": "A VS Code snippet for LangChain ChatOpenAI with dotenv and a predefined prompt."
	},

	"langchainPromptDemo": {
		"prefix": "d1-key-prompt",
		"body": [
		"// read prompt from keyboard",
		"// üü¢ npm i readline-promise",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"let prompt = await rl.question('How can I help you: ')",
		"// üü¢ short prompts only for demos + model: \"gpt-4\" ; why smaller is better for dev https://x.com/js_craft_hq/status/1872580473567994105",
		"let response = await model.invoke(prompt)",
		"console.log(response)",
		"// üü¢ Give me an easy trivia question from math",
		"rl.close()"
		],
		"description": "A VS Code snippet for a LangChain ChatOpenAI demo using readline-promise and dotenv."
	}, 
	

"d1-prompt-templates": {
    "prefix": "d1-prompt-templates",
    "body": [
      "// adding prompt templates",
      "import { ChatOpenAI } from \"@langchain/openai\"",
      "import * as dotenv from \"dotenv\"",
      "import * as readline from 'node:readline/promises'",
      "import { stdin as input, stdout as output } from 'node:process'",
      "import { PromptTemplate } from \"@langchain/core/prompts\"",
      "",
      "dotenv.config()",
      "let model = new ChatOpenAI()",
      "const prompt = new PromptTemplate({",
      "    inputVariables: [ \"level\", \"domain\"],",
      "    template: \"Give me a {level} trivia question from {domain}\"",
      "})",
      "",
      "let rl = readline.createInterface({ input, output })",
      "let level = await rl.question('üìä Question level: ')",
      "let domain = await rl.question('üìñ Question domain: ')",
      "",
      "// üü¢ why is this async?",
      "const formattedPrompt = await prompt.format({level, domain})",
      "let response = await model.invoke(formattedPrompt)",
      "console.log(response)",
      "rl.close()",
      "// üü¢ see tokenUsage; 1 token is close to 1 word ",
    ],
    "description": "A VS Code snippet for LangChain ChatOpenAI with prompt templates, chains, and string output parser."
  }, 


"d1-chains-parsers": {
	"prefix": "d1-chains-parsers",
	"body": [
	"// adding chains and string output parser",
	"import { ChatOpenAI } from \"@langchain/openai\"",
	"import * as dotenv from \"dotenv\"",
	"import * as readline from 'node:readline/promises'",
	"import { stdin as input, stdout as output } from 'node:process'",
	"import { PromptTemplate } from \"@langchain/core/prompts\"",
	"import { StringOutputParser} from \"@langchain/core/output_parsers\"",
	"",
	"dotenv.config()",
	"let model = new ChatOpenAI()",
	"let rl = readline.createInterface({ input, output })",
	"",
	"const prompt = new PromptTemplate({",
	"    inputVariables: [ \"level\", \"domain\"],",
	"    template: \"Give me a {level} trivia question from {domain}\"",
	"})",
	"// üü¢ add ouput parser here / pipe LCEL",
	"const chain = prompt.pipe(model)",
	"",
	"let level = await rl.question('üìä Question level: ')",
	"let domain = await rl.question('üìñ Question domain: ')",
	"",
	"let question = await chain.invoke({level, domain})",
	"console.log(question)",
	"",
	"rl.close()"
	],
	"description": "d1-chains-parsers"
},
  
"d1-mutiple-chains-comma-parser": {
	"prefix": "d1-mutiple-chains-comma-parser",
	"body": [
	"// mutiple chains and CommaSeparatedListOutputParser",
	"import { ChatOpenAI } from \"@langchain/openai\"",
	"import * as dotenv from \"dotenv\"",
	"import * as readline from 'node:readline/promises'",
	"import { stdin as input, stdout as output } from 'node:process'",
	"import { PromptTemplate } from \"@langchain/core/prompts\"",
	"import { StringOutputParser, CommaSeparatedListOutputParser} from \"@langchain/core/output_parsers\"",
	"",
	"dotenv.config()",
	"let model = new ChatOpenAI()",
	"let rl = readline.createInterface({ input, output })",
	"",
	"const qPrompt = new PromptTemplate({",
	"    inputVariables: [ \"level\", \"domain\"],",
	"    template: \"Give me a {level} trivia question from {domain}\"",
	"})",
	"",
	"const qChain = qPrompt.pipe(model).pipe(new StringOutputParser())",
	"",
	"let level = await rl.question('üìä Question level: ')",
	"let domain = await rl.question('üìñ Question domain: ')",
	"// üü¢ chains - one output becomes the next input",
	"let question = await qChain.invoke({level, domain})",
	"console.log(question)",
	"",
	"const aPrompt = new PromptTemplate({",
	"    inputVariables: [ \"question\"],",
	"    template: \"Give 4 possible answers for {question}, separated by commas, 3 false and 1 correct, in a random order.\"",
	"})",
	"// üü¢ output parser here",
	"const aChain = aPrompt.pipe(model)",
	"",
	"let answers = await aChain.invoke({question})",
	"console.log(answers)",
	"// üü¢ we can use answers with a for each answers.forEach( (q, i) => console.log(i + \" \" + q))",
	"",
    "rl.close()"
	],
	"description": "d1-mutiple-chains-comma-parser"
},


"d1-structured-zod": {
	  "prefix": "d1-structured-zod",
	  "body": [
		"// StructuredOutputParser and ZOD",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"import { PromptTemplate } from \"@langchain/core/prompts\"",
		"import { StructuredOutputParser } from \"langchain/output_parsers\"",
		"import { z } from \"zod\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"// üü¢ in python this is Pydantic",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    {format_instructions}`",
		")",
		"",
		"const chain = prompt.pipe(model).pipe(parser)",
		"// üü¢ show this; parser.getFormatInstructions(); parsers are just part of the prompt",
		"let data = await chain.invoke({",
		"    format_instructions: parser.getFormatInstructions()",
		"})",
		"",
		"console.log(data)",
		"",
		"rl.close()"
	  ],
	  "description": "d1-structured-zod"
},


"d1-mem1-dowhile": {
	  "prefix": "d1-mem1-dowhile",
	  "body": [
		"// add memory part 1 + DO WHILE",
		"// üü¢ show GPT Conversation with the most expensive painting the the world ? + where it that?",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"// üü¢ ChatPromptTemplate vs PromptTemplate; why ?",
		"import { ChatPromptTemplate, PromptTemplate } from \"@langchain/core/prompts\"",
		"import { JsonOutputParser, StructuredOutputParser} from \"@langchain/core/output_parsers\"",
		"import { z } from \"zod\"",
		"import { MessagesPlaceholder } from \"@langchain/core/prompts\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"// üü¢ aks in prompt not to repeat the questions",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    Don't repeat previous questions \\n",
		"    {format_instructions}`",
		")",
		"",
		"const formattedPrompt = await prompt.format({",
		"    format_instructions: parser.getFormatInstructions()",
		"});",
		"",
		"const chatHistory = []",
		"",
		"// üü¢ a ChatPromptTemplate must have chat_history",
		"const chatPromptTemplate = ChatPromptTemplate.fromMessages([",
		"    new MessagesPlaceholder(\"chat_history\"),",
		"    [\"human\", \"{input}\"]",
		"])",
		"",
		"// üü¢ JsonOutputParser",
		"const chain = chatPromptTemplate.pipe(model).pipe(new JsonOutputParser())",
		"",
		"let oneMoreQuestion",
		"do {",
		"    const data = await chain.invoke({",
		"        input: formattedPrompt,",
		"        chat_history: chatHistory",
		"    })",
		"    console.log(data)",
		"    oneMoreQuestion = await rl.question('üíØ Ask one more question (y for yes):')",
		"} while(oneMoreQuestion == 'y')",
		"rl.close()"
	  ],
	  "description": "d1-mem1-dowhile"
},


"d1-mem2": {
	  "prefix": "d1-mem2",
	  "body": [
		"// keeping track of memory",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"import { ChatPromptTemplate, PromptTemplate } from \"@langchain/core/prompts\"",
		"import { JsonOutputParser, StructuredOutputParser} from \"@langchain/core/output_parsers\"",
		"import { z } from \"zod\"",
		"import { MessagesPlaceholder } from \"@langchain/core/prompts\"",
		"import { HumanMessage, AIMessage } from \"@langchain/core/messages\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    Don't repeat previous questions \\n",
		"    {format_instructions}`",
		")",
		"",
		"const formattedPrompt = await prompt.format({",
		"    format_instructions: parser.getFormatInstructions()",
		"});",
		"",
		"const chatHistory = []",
		"",
		"const chatPromptTemplate = ChatPromptTemplate.fromMessages([",
		"    new MessagesPlaceholder(\"chat_history\"),",
		"    [\"human\", \"{input}\"]",
		"])",
		"",
		"const chain = chatPromptTemplate.pipe(model).pipe(new JsonOutputParser())",
		"",
		"let oneMoreQuestion",
		"do {",
		"    const data = await chain.invoke({",
		"        input: formattedPrompt,",
		"        chat_history: chatHistory",
		"    })",
		"    console.log(data)",
		"// üü¢ HumanMessage and AIMessage",
		"    chatHistory.push(new HumanMessage(formattedPrompt))",
		"    chatHistory.push(new AIMessage(JSON.stringify(data)))",
		"    oneMoreQuestion = await rl.question('üíØ Ask one more question (y for yes):')",
		"} while(oneMoreQuestion == 'y')",
		"rl.close()"
	  ],
	  "description": "d1-mem2"
},

	"d2-start": {
	  "prefix": "d2-start",
	  "body": [
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { Document } from \"@langchain/core/documents\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"const model = new ChatOpenAI()",
		"",
		"// üü¢ training cutoff date + private data",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		" const documentA = new Document({",
		"  pageContent:",
		"    `LangSmith is a unified DevOps platform for developing, ",
		"    collaborating, testing, deploying, and monitoring ",
		"    LLM applications.`",
		"})",
		"",
		"const documentB = new Document({",
		"  pageContent: `LangSmith was first launched in closed beta in July 2023`",
		"})",
		"",
		"const chain = await createStuffDocumentsChain({",
		"    llm: model,",
		"    prompt,",
		"})",
		"",
		"// üü¢ LangSmith is a Dog! LangSmith was born in 2021!",
		"let question = 'What is LangSmith?'",
		"const data = await chain.invoke({",
		"    input: question, ", 
		"    context: [documentA, documentB]",
		"})",
		"",
		"console.log(data)"
	  ],
	  "description": "Snippet for starting a LangChain integration using ChatOpenAI and context documents."
	},



"d2-web-loaders": {
		  "prefix": "d2-web-loaders",
		  "body": [
			"// online documents loader",
			"// ‚õîÔ∏è import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";",
			"import { ChatOpenAI } from \"@langchain/openai\"",
			"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
			"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
			"//üü¢ document loader that can retrive the content of a web page",
			"import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\"",
			"//üü¢ tool for making the vectors and embeddings",
			"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
			"import { OpenAIEmbeddings } from \"@langchain/openai\"",
			"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
			"//üü¢ retrieval tool",
			"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
			"import * as dotenv from \"dotenv\"",
			"",
			"dotenv.config()",
			"",
			"const model = new ChatOpenAI({})",
			"",
			"const prompt = ChatPromptTemplate.fromTemplate(",
			"    `Answer the user's question from the following context: ",
			"    {context}",
			"    Question: {input}`",
			")",
			"",
			"let retrievalChain, splitDocs",
			"",
			"//üü¢ the RAG process",
			"async function loadDocumentsFromUrl(url) {",
			"    //üü¢ document loaders",
			"    const loader = new CheerioWebBaseLoader(url)",
			"    const docs = await loader.load()",
			"",
			"    //üü¢ document transformers",
			"    const splitter = new RecursiveCharacterTextSplitter({",
			"        chunkSize: 100,",
			"        chunkOverlap: 20,",
			"    })",
			"",
			"    splitDocs = await splitter.splitDocuments(docs)",
			"",
			"    //üü¢ setting up the embeddings ",
			"    const embeddings = new OpenAIEmbeddings()",
			"",
			"    //üü¢ making a local vector DB",
			"    const vectorstore = await MemoryVectorStore.fromDocuments(",
			"        splitDocs,",
			"        embeddings",
			"    )",
			"    ",
			"    //üü¢ what we use to fetch data from the vector DB ",
			"    const retriever = vectorstore.asRetriever()",
			"",
			"    const chain = await createStuffDocumentsChain({",
			"        llm: model,",
			"        prompt",
			"    })",
			"",
			"    retrievalChain = await createRetrievalChain({",
			"        combineDocsChain: chain",
			"        retriever",
			"    })",
			"}",
			"",
			"await loadDocumentsFromUrl(\"https://www.js-craft.io/about/\")",
			"",
			"console.log(\"‚úÖ document loaded\")",
			"",
			"const data = await retrievalChain.invoke({",
			"    input: \"What is the name of Daniel's cat?\",",
			"    context: splitDocs",
			"})",
			"",
			"console.log(data)"
		  ],
		  "description": "Snippet for creating a web document loader, splitting documents, and retrieval using LangChain."
},

"d2-pdf-loader": {
	  "prefix": "d2-pdf-loader",
	  "body": [
		"// ‚õîÔ∏è npm i @langchain/community @langchain/core pdf-parse",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"//üü¢ PDFLoader",
		"import { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\"",
		"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
		"import { OpenAIEmbeddings } from \"@langchain/openai\"",
		"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
		"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"",
		"const model = new ChatOpenAI({})",
		"",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		"let retrievalChain, splitDocs",
		"",
		"//üü¢ loadDocumentsFromPDF",
		"async function loadDocumentsFromPDF(url) {",
		"    const loader = new PDFLoader(url)",
		"    const docs = await loader.load()",
		"",
		"    const splitter = new RecursiveCharacterTextSplitter({",
		"        chunkSize: 100,",
		"        chunkOverlap: 20,",
		"    })",
		"",
		"    splitDocs = await splitter.splitDocuments(docs)",
		"",
		"    const embeddings = new OpenAIEmbeddings()",
		"",
		"    const vectorstore = await MemoryVectorStore.fromDocuments(",
		"        splitDocs,",
		"        embeddings",
		"    )",
		"    ",
		"    const retriever = vectorstore.asRetriever()",
		"",
		"    const chain = await createStuffDocumentsChain({",
		"        llm: model,",
		"        prompt",
		"    })",
		"",
		"    retrievalChain = await createRetrievalChain({",
		"        combineDocsChain: chain,-",
		"        retriever",
		"    })",
		"}",
		"",
		"//üü¢ loadDocumentsFromPDF",
		"await loadDocumentsFromPDF(\"daniel.pdf\")",
		"",
		"console.log(\"‚úÖ document loaded\")",
		"",
		"const data = await retrievalChain.invoke({",
		"    input: \"What is the name of Daniel's cat?\",",
		"    context: splitDocs",
		"})",
		"",
		"console.log(data)"
	  ],
	  "description": "Snippet for loading and processing PDF documents with LangChain."
},
  

"d2-story": {
	  "prefix": "d2-story",
	  "body": [
		"// storyPrompt and chain ",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"import { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\"",
		"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
		"import { OpenAIEmbeddings } from \"@langchain/openai\"",
		"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
		"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
		"//üü¢ PromptTemplate and StringOutputParser",
		"import { PromptTemplate } from \"@langchain/core/prompts\"",
		"import { StringOutputParser} from \"@langchain/core/output_parsers\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"",
		"const model = new ChatOpenAI({})",
		"",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		"let retrievalChain, splitDocs",
		"",
		"async function loadDocumentsFromPDF(url) {",
		"    const loader = new PDFLoader(url)",
		"    const docs = await loader.load()",
		"",
		"    const splitter = new RecursiveCharacterTextSplitter({",
		"        chunkSize: 100,",
		"        chunkOverlap: 20,",
		"    })",
		"",
		"    splitDocs = await splitter.splitDocuments(docs)",
		"",
		"    const embeddings = new OpenAIEmbeddings()",
		"",
		"    const vectorstore = await MemoryVectorStore.fromDocuments(",
		"        splitDocs,",
		"        embeddings",
		"    )",
		"    ",
		"    const retriever = vectorstore.asRetriever()",
		"",
		"    const chain = await createStuffDocumentsChain({",
		"        llm: model,",
		"        prompt",
		"    })",
		"",
		"    retrievalChain = await createRetrievalChain({",
		"        combineDocsChain: chain,",
		"        retriever",
		"    })",
		"}",
		"",
		"await loadDocumentsFromPDF(\"daniel.pdf\")",
		"",
		"console.log(\"‚úÖ document loaded\")",
		"",
		"const {answer} = await retrievalChain.invoke({",
		"    input: \"What is the name of Daniel's cat?\",",
		"    context: splitDocs",
		"})",
		"",
		"console.log(answer)",
		"",
		"//üü¢ storyPrompt and chain",
		"const storyPrompt = new PromptTemplate({",
		"    inputVariables: [ \"sentence\"],",
		"    template: \"Tell me a story based on the characters from this sentence: {sentence}\"",
		"})",
		"",
		"const chain = storyPrompt.pipe(model).pipe(new StringOutputParser())",
		"",
		"let story = await chain.invoke({sentence: answer})",
		"console.log(story)"
	  ],
	  "description": "Snippet for creating a story prompt and chain using LangChain."
},

"d2-streams": {
	  "prefix": "d2-streams",
	  "body": [
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"import { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\"",
		"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
		"import { OpenAIEmbeddings } from \"@langchain/openai\"",
		"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
		"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
		"import { PromptTemplate } from \"@langchain/core/prompts\"",
		"import { StringOutputParser } from \"@langchain/core/output_parsers\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"",
		"const model = new ChatOpenAI({})",
		"",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		"let retrievalChain, splitDocs",
		"",
		"async function loadDocumentsFromPDF(url) {",
		"    const loader = new PDFLoader(url)",
		"    const docs = await loader.load()",
		"",
		"    const splitter = new RecursiveCharacterTextSplitter({",
		"        chunkSize: 100,",
		"        chunkOverlap: 20,",
		"    })",
		"",
		"    splitDocs = await splitter.splitDocuments(docs)",
		"",
		"    const embeddings = new OpenAIEmbeddings()",
		"",
		"    const vectorstore = await MemoryVectorStore.fromDocuments(",
		"        splitDocs,",
		"        embeddings",
		"    )",
		"    ",
		"    const retriever = vectorstore.asRetriever()",
		"",
		"    const chain = await createStuffDocumentsChain({",
		"        llm: model,",
		"        prompt",
		"    })",
		"",
		"    retrievalChain = await createRetrievalChain({",
		"        combineDocsChain: chain,",
		"        retriever",
		"    })",
		"}",
		"",
		"await loadDocumentsFromPDF(\"daniel.pdf\")",
		"",
		"console.log(\"‚úÖ document loaded\")",
		"",
		"const {answer} = await retrievalChain.invoke({",
		"    input: \"What is the name of Daniel's cat?\",",
		"    context: splitDocs",
		"})",
		"",
		"console.log(answer)",
		"",
		"const storyPrompt = new PromptTemplate({",
		"    inputVariables: [ \"sentence\"],",
		"    template: \"Tell me a story based on the characters from this sentence: {sentence}\"",
		"})",
		"",
		"const chain = storyPrompt.pipe(model).pipe(new StringOutputParser())",
		"",
		"// let story = await chain.invoke({sentence: answer})",
		"// console.log(story)",
		"// üü¢ invoke VS streaming",
		"// üü¢ LLMs autocomplete one token at a time; if 2 + 2 = 5 they will try to justify ",
		"const stream = await chain.stream({sentence: answer})",
		"const chunks = [];",
		"for await (const chunk of stream) {",
		"  chunks.push(chunk);",
		"  // console.log(chunk)",
		"  process.stdout.write(chunk)",
		"}"
	  ],
	  "description": "Snippet for streaming a story based on characters in LangChain."
}
  
  
	  
  
}