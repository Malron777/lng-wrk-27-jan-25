{
	// Place your workshop-code workspace snippets here. Each snippet is defined under a snippet name and has a scope, prefix, body and 
	// description. Add comma separated ids of the languages where the snippet is applicable in the scope field. If scope 
	// is left empty or omitted, the snippet gets applied to all languages. The prefix is what is 
	// used to trigger the snippet and the body will be expanded and inserted. Possible variables are: 
	// $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders. 
	// Placeholders with the same ids are connected.
	// Example:
	// "Print to console": {
	// 	"scope": "javascript,typescript",
	// 	"prefix": "log",
	// 	"body": [
	// 		"console.log('$1');",
	// 		"$2"
	// 	],
	// 	"description": "Log output to console"
	// }

	"d1-start": {
		"prefix": "d1-start",
		"body": [
		  "import { ChatOpenAI } from \"@langchain/openai\"",
		  "import * as dotenv from \"dotenv\"",
		  "// ðŸŸ¢ note the .env file -> https://platform.openai.com/api-keys",
		  "dotenv.config()",
		  "// ðŸŸ¢ openAIApiKey, temperature ",
		  "let model = new ChatOpenAI()",
		  "let response = await model.invoke('What is the age of Bugs Bunny?')",
		  "console.log(response)",
		  "// ðŸŸ¢ node --no-deprecation index.js"
		],
		"description": "A VS Code snippet for LangChain ChatOpenAI with dotenv and a predefined prompt."
	},

	"langchainPromptDemo": {
		"prefix": "d1-key-prompt",
		"body": [
		"// read prompt from keyboard",
		"// ðŸŸ¢ npm i readline-promise",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"let prompt = await rl.question('How can I help you: ')",
		"// ðŸŸ¢ short prompts only for demos + model: \"gpt-4\" ; why smaller is better for dev https://x.com/js_craft_hq/status/1872580473567994105",
		"let response = await model.invoke(prompt)",
		"console.log(response)",
		"// ðŸŸ¢ Give me an easy trivia question from math",
		"rl.close()"
		],
		"description": "A VS Code snippet for a LangChain ChatOpenAI demo using readline-promise and dotenv."
	}, 
	

"d1-prompt-templates": {
    "prefix": "d1-prompt-templates",
    "body": [
      "// adding prompt templates",
      "import { ChatOpenAI } from \"@langchain/openai\"",
      "import * as dotenv from \"dotenv\"",
      "import * as readline from 'node:readline/promises'",
      "import { stdin as input, stdout as output } from 'node:process'",
      "import { PromptTemplate } from \"@langchain/core/prompts\"",
      "",
      "dotenv.config()",
      "let model = new ChatOpenAI()",
      "const prompt = new PromptTemplate({",
      "    inputVariables: [ \"level\", \"domain\"],",
      "    template: \"Give me a {level} trivia question from {domain}\"",
      "})",
      "",
      "let rl = readline.createInterface({ input, output })",
      "let level = await rl.question('ðŸ“Š Question level: ')",
      "let domain = await rl.question('ðŸ“– Question domain: ')",
      "",
      "// ðŸŸ¢ why is this async?",
      "const formattedPrompt = await prompt.format({level, domain})",
      "let response = await model.invoke(formattedPrompt)",
      "console.log(response)",
      "rl.close()",
      "// ðŸŸ¢ see tokenUsage; 1 token is close to 1 word ",
    ],
    "description": "A VS Code snippet for LangChain ChatOpenAI with prompt templates, chains, and string output parser."
  }, 


"d1-chains-parsers": {
	"prefix": "d1-chains-parsers",
	"body": [
	"// adding chains and string output parser",
	"import { ChatOpenAI } from \"@langchain/openai\"",
	"import * as dotenv from \"dotenv\"",
	"import * as readline from 'node:readline/promises'",
	"import { stdin as input, stdout as output } from 'node:process'",
	"import { PromptTemplate } from \"@langchain/core/prompts\"",
	"import { StringOutputParser} from \"@langchain/core/output_parsers\"",
	"",
	"dotenv.config()",
	"let model = new ChatOpenAI()",
	"let rl = readline.createInterface({ input, output })",
	"",
	"const prompt = new PromptTemplate({",
	"    inputVariables: [ \"level\", \"domain\"],",
	"    template: \"Give me a {level} trivia question from {domain}\"",
	"})",
	"// ðŸŸ¢ add ouput parser here / pipe LCEL",
	"const chain = prompt.pipe(model)",
	"",
	"let level = await rl.question('ðŸ“Š Question level: ')",
	"let domain = await rl.question('ðŸ“– Question domain: ')",
	"",
	"let question = await chain.invoke({level, domain})",
	"console.log(question)",
	"",
	"rl.close()"
	],
	"description": "d1-chains-parsers"
},
  
"d1-mutiple-chains-comma-parser": {
	"prefix": "d1-mutiple-chains-comma-parser",
	"body": [
	"// mutiple chains and CommaSeparatedListOutputParser",
	"import { ChatOpenAI } from \"@langchain/openai\"",
	"import * as dotenv from \"dotenv\"",
	"import * as readline from 'node:readline/promises'",
	"import { stdin as input, stdout as output } from 'node:process'",
	"import { PromptTemplate } from \"@langchain/core/prompts\"",
	"import { StringOutputParser, CommaSeparatedListOutputParser} from \"@langchain/core/output_parsers\"",
	"",
	"dotenv.config()",
	"let model = new ChatOpenAI()",
	"let rl = readline.createInterface({ input, output })",
	"",
	"const qPrompt = new PromptTemplate({",
	"    inputVariables: [ \"level\", \"domain\"],",
	"    template: \"Give me a {level} trivia question from {domain}\"",
	"})",
	"",
	"const qChain = qPrompt.pipe(model).pipe(new StringOutputParser())",
	"",
	"let level = await rl.question('ðŸ“Š Question level: ')",
	"let domain = await rl.question('ðŸ“– Question domain: ')",
	"// ðŸŸ¢ chains - one output becomes the next input",
	"let question = await qChain.invoke({level, domain})",
	"console.log(question)",
	"",
	"const aPrompt = new PromptTemplate({",
	"    inputVariables: [ \"question\"],",
	"    template: \"Give 4 possible answers for {question}, separated by commas, 3 false and 1 correct, in a random order.\"",
	"})",
	"// ðŸŸ¢ output parser here",
	"const aChain = aPrompt.pipe(model)",
	"",
	"let answers = await aChain.invoke({question})",
	"console.log(answers)",
	"// ðŸŸ¢ we can use answers with a for each answers.forEach( (q, i) => console.log(i + \" \" + q))",
	"",
    "rl.close()"
	],
	"description": "d1-mutiple-chains-comma-parser"
},


"d1-structured-zod": {
	  "prefix": "d1-structured-zod",
	  "body": [
		"// StructuredOutputParser and ZOD",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"import { PromptTemplate } from \"@langchain/core/prompts\"",
		"import { StructuredOutputParser } from \"langchain/output_parsers\"",
		"import { z } from \"zod\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"// ðŸŸ¢ in python this is Pydantic",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    {format_instructions}`",
		")",
		"",
		"const chain = prompt.pipe(model).pipe(parser)",
		"// ðŸŸ¢ show this; parser.getFormatInstructions(); parsers are just part of the prompt",
		"let data = await chain.invoke({",
		"    format_instructions: parser.getFormatInstructions()",
		"})",
		"",
		"console.log(data)",
		"",
		"rl.close()"
	  ],
	  "description": "d1-structured-zod"
},


"d1-mem1-dowhile": {
	  "prefix": "d1-mem1-dowhile",
	  "body": [
		"// add memory part 1 + DO WHILE",
		"// ðŸŸ¢ show GPT Conversation with the most expensive painting the the world ? + where it that?",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"// ðŸŸ¢ ChatPromptTemplate vs PromptTemplate; why ?",
		"import { ChatPromptTemplate, PromptTemplate } from \"@langchain/core/prompts\"",
		"import { JsonOutputParser, StructuredOutputParser} from \"@langchain/core/output_parsers\"",
		"import { z } from \"zod\"",
		"import { MessagesPlaceholder } from \"@langchain/core/prompts\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"// ðŸŸ¢ aks in prompt not to repeat the questions",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    Don't repeat previous questions \\n",
		"    {format_instructions}`",
		")",
		"",
		"const formattedPrompt = await prompt.format({",
		"    format_instructions: parser.getFormatInstructions()",
		"});",
		"",
		"const chatHistory = []",
		"",
		"// ðŸŸ¢ a ChatPromptTemplate must have chat_history",
		"const chatPromptTemplate = ChatPromptTemplate.fromMessages([",
		"    new MessagesPlaceholder(\"chat_history\"),",
		"    [\"human\", \"{input}\"]",
		"])",
		"",
		"// ðŸŸ¢ JsonOutputParser",
		"const chain = chatPromptTemplate.pipe(model).pipe(new JsonOutputParser())",
		"",
		"let oneMoreQuestion",
		"do {",
		"    const data = await chain.invoke({",
		"        input: formattedPrompt,",
		"        chat_history: chatHistory",
		"    })",
		"    console.log(data)",
		"    oneMoreQuestion = await rl.question('ðŸ’¯ Ask one more question (y for yes):')",
		"} while(oneMoreQuestion == 'y')",
		"rl.close()"
	  ],
	  "description": "d1-mem1-dowhile"
},


"d1-mem2": {
	  "prefix": "d1-mem2",
	  "body": [
		"// keeping track of memory",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import * as dotenv from \"dotenv\"",
		"import * as readline from 'node:readline/promises'",
		"import { stdin as input, stdout as output } from 'node:process'",
		"import { ChatPromptTemplate, PromptTemplate } from \"@langchain/core/prompts\"",
		"import { JsonOutputParser, StructuredOutputParser} from \"@langchain/core/output_parsers\"",
		"import { z } from \"zod\"",
		"import { MessagesPlaceholder } from \"@langchain/core/prompts\"",
		"import { HumanMessage, AIMessage } from \"@langchain/core/messages\"",
		"",
		"dotenv.config()",
		"let model = new ChatOpenAI()",
		"let rl = readline.createInterface({ input, output })",
		"",
		"const parser = StructuredOutputParser.fromZodSchema(",
		"    z.object({",
		"        question: z.string().describe(",
		"            `tell me a random geography trivia question`",
		"        ),",
		"        answers: z",
		"            .array(z.string())",
		"            .describe(`",
		"                give 4 possible answers, in a random order, ",
		"                out of which only one is true.`",
		"            ),",
		"        correctIndex: z.number().describe(",
		"            `the number of the correct answer, zero indexed`",
		"        ),",
		"    })",
		")",
		"",
		"const prompt = PromptTemplate.fromTemplate(",
		"    `Answer the user's question as best as possible.\\n",
		"    Don't repeat previous questions \\n",
		"    {format_instructions}`",
		")",
		"",
		"const formattedPrompt = await prompt.format({",
		"    format_instructions: parser.getFormatInstructions()",
		"});",
		"",
		"const chatHistory = []",
		"",
		"const chatPromptTemplate = ChatPromptTemplate.fromMessages([",
		"    new MessagesPlaceholder(\"chat_history\"),",
		"    [\"human\", \"{input}\"]",
		"])",
		"",
		"const chain = chatPromptTemplate.pipe(model).pipe(new JsonOutputParser())",
		"",
		"let oneMoreQuestion",
		"do {",
		"    const data = await chain.invoke({",
		"        input: formattedPrompt,",
		"        chat_history: chatHistory",
		"    })",
		"    console.log(data)",
		"// ðŸŸ¢ HumanMessage and AIMessage",
		"    chatHistory.push(new HumanMessage(formattedPrompt))",
		"    chatHistory.push(new AIMessage(JSON.stringify(data)))",
		"    oneMoreQuestion = await rl.question('ðŸ’¯ Ask one more question (y for yes):')",
		"} while(oneMoreQuestion == 'y')",
		"rl.close()"
	  ],
	  "description": "d1-mem2"
},

	"d2-start": {
	  "prefix": "d2-start",
	  "body": [
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { Document } from \"@langchain/core/documents\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"const model = new ChatOpenAI()",
		"",
		"// ðŸŸ¢ training cutoff date + private data",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		" const documentA = new Document({",
		"  pageContent:",
		"    `LangSmith is a unified DevOps platform for developing, ",
		"    collaborating, testing, deploying, and monitoring ",
		"    LLM applications.`",
		"})",
		"",
		"const documentB = new Document({",
		"  pageContent: `LangSmith was first launched in closed beta in July 2023`",
		"})",
		"",
		"const chain = await createStuffDocumentsChain({",
		"    llm: model,",
		"    prompt,",
		"})",
		"",
		"// ðŸŸ¢ LangSmith is a Dog! LangSmith was born in 2021!",
		"let question = 'What is LangSmith?'",
		"const data = await chain.invoke({",
		"    input: question, ", 
		"    context: [documentA, documentB]",
		"})",
		"",
		"console.log(data)"
	  ],
	  "description": "Snippet for starting a LangChain integration using ChatOpenAI and context documents."
	},



"d2-web-loaders": {
		  "prefix": "d2-web-loaders",
		  "body": [
			"// online documents loader",
			"// â›”ï¸ import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";",
			"import { ChatOpenAI } from \"@langchain/openai\"",
			"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
			"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
			"//ðŸŸ¢ document loader that can retrive the content of a web page",
			"import { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\"",
			"//ðŸŸ¢ tool for making the vectors and embeddings",
			"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
			"import { OpenAIEmbeddings } from \"@langchain/openai\"",
			"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
			"//ðŸŸ¢ retrieval tool",
			"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
			"import * as dotenv from \"dotenv\"",
			"",
			"dotenv.config()",
			"",
			"const model = new ChatOpenAI({})",
			"",
			"const prompt = ChatPromptTemplate.fromTemplate(",
			"    `Answer the user's question from the following context: ",
			"    {context}",
			"    Question: {input}`",
			")",
			"",
			"let retrievalChain, splitDocs",
			"",
			"//ðŸŸ¢ the RAG process",
			"async function loadDocumentsFromUrl(url) {",
			"    //ðŸŸ¢ document loaders",
			"    const loader = new CheerioWebBaseLoader(url)",
			"    const docs = await loader.load()",
			"",
			"    //ðŸŸ¢ document transformers",
			"    const splitter = new RecursiveCharacterTextSplitter({",
			"        chunkSize: 100,",
			"        chunkOverlap: 20,",
			"    })",
			"",
			"    splitDocs = await splitter.splitDocuments(docs)",
			"",
			"    //ðŸŸ¢ setting up the embeddings ",
			"    const embeddings = new OpenAIEmbeddings()",
			"",
			"    //ðŸŸ¢ making a local vector DB",
			"    const vectorstore = await MemoryVectorStore.fromDocuments(",
			"        splitDocs,",
			"        embeddings",
			"    )",
			"    ",
			"    //ðŸŸ¢ what we use to fetch data from the vector DB ",
			"    const retriever = vectorstore.asRetriever()",
			"",
			"    const chain = await createStuffDocumentsChain({",
			"        llm: model,",
			"        prompt",
			"    })",
			"",
			"    retrievalChain = await createRetrievalChain({",
			"        combineDocsChain: chain",
			"        retriever",
			"    })",
			"}",
			"",
			"await loadDocumentsFromUrl(\"https://www.js-craft.io/about/\")",
			"",
			"console.log(\"âœ… document loaded\")",
			"",
			"const data = await retrievalChain.invoke({",
			"    input: \"What is the name of Daniel's cat?\",",
			"    context: splitDocs",
			"})",
			"",
			"console.log(data)"
		  ],
		  "description": "Snippet for creating a web document loader, splitting documents, and retrieval using LangChain."
},

"d2-pdf-loader": {
	  "prefix": "d2-pdf-loader",
	  "body": [
		"// â›”ï¸ npm i @langchain/community @langchain/core pdf-parse",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"//ðŸŸ¢ PDFLoader",
		"import { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\"",
		"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
		"import { OpenAIEmbeddings } from \"@langchain/openai\"",
		"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
		"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"",
		"const model = new ChatOpenAI({})",
		"",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		"let retrievalChain, splitDocs",
		"",
		"//ðŸŸ¢ loadDocumentsFromPDF",
		"async function loadDocumentsFromPDF(url) {",
		"    const loader = new PDFLoader(url)",
		"    const docs = await loader.load()",
		"",
		"    const splitter = new RecursiveCharacterTextSplitter({",
		"        chunkSize: 100,",
		"        chunkOverlap: 20,",
		"    })",
		"",
		"    splitDocs = await splitter.splitDocuments(docs)",
		"",
		"    const embeddings = new OpenAIEmbeddings()",
		"",
		"    const vectorstore = await MemoryVectorStore.fromDocuments(",
		"        splitDocs,",
		"        embeddings",
		"    )",
		"    ",
		"    const retriever = vectorstore.asRetriever()",
		"",
		"    const chain = await createStuffDocumentsChain({",
		"        llm: model,",
		"        prompt",
		"    })",
		"",
		"    retrievalChain = await createRetrievalChain({",
		"        combineDocsChain: chain,-",
		"        retriever",
		"    })",
		"}",
		"",
		"//ðŸŸ¢ loadDocumentsFromPDF",
		"await loadDocumentsFromPDF(\"daniel.pdf\")",
		"",
		"console.log(\"âœ… document loaded\")",
		"",
		"const data = await retrievalChain.invoke({",
		"    input: \"What is the name of Daniel's cat?\",",
		"    context: splitDocs",
		"})",
		"",
		"console.log(data)"
	  ],
	  "description": "Snippet for loading and processing PDF documents with LangChain."
},
  

"d2-story": {
	  "prefix": "d2-story",
	  "body": [
		"// storyPrompt and chain ",
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"import { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\"",
		"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
		"import { OpenAIEmbeddings } from \"@langchain/openai\"",
		"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
		"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
		"//ðŸŸ¢ PromptTemplate and StringOutputParser",
		"import { PromptTemplate } from \"@langchain/core/prompts\"",
		"import { StringOutputParser} from \"@langchain/core/output_parsers\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"",
		"const model = new ChatOpenAI({})",
		"",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		"let retrievalChain, splitDocs",
		"",
		"async function loadDocumentsFromPDF(url) {",
		"    const loader = new PDFLoader(url)",
		"    const docs = await loader.load()",
		"",
		"    const splitter = new RecursiveCharacterTextSplitter({",
		"        chunkSize: 100,",
		"        chunkOverlap: 20,",
		"    })",
		"",
		"    splitDocs = await splitter.splitDocuments(docs)",
		"",
		"    const embeddings = new OpenAIEmbeddings()",
		"",
		"    const vectorstore = await MemoryVectorStore.fromDocuments(",
		"        splitDocs,",
		"        embeddings",
		"    )",
		"    ",
		"    const retriever = vectorstore.asRetriever()",
		"",
		"    const chain = await createStuffDocumentsChain({",
		"        llm: model,",
		"        prompt",
		"    })",
		"",
		"    retrievalChain = await createRetrievalChain({",
		"        combineDocsChain: chain,",
		"        retriever",
		"    })",
		"}",
		"",
		"await loadDocumentsFromPDF(\"daniel.pdf\")",
		"",
		"console.log(\"âœ… document loaded\")",
		"",
		"const {answer} = await retrievalChain.invoke({",
		"    input: \"What is the name of Daniel's cat?\",",
		"    context: splitDocs",
		"})",
		"",
		"console.log(answer)",
		"",
		"//ðŸŸ¢ storyPrompt and chain",
		"const storyPrompt = new PromptTemplate({",
		"    inputVariables: [ \"sentence\"],",
		"    template: \"Tell me a story based on the characters from this sentence: {sentence}\"",
		"})",
		"",
		"const chain = storyPrompt.pipe(model).pipe(new StringOutputParser())",
		"",
		"let story = await chain.invoke({sentence: answer})",
		"console.log(story)"
	  ],
	  "description": "Snippet for creating a story prompt and chain using LangChain."
},

"d2-streams": {
	  "prefix": "d2-streams",
	  "body": [
		"import { ChatOpenAI } from \"@langchain/openai\"",
		"import { ChatPromptTemplate } from \"@langchain/core/prompts\"",
		"import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"",
		"import { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\"",
		"import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"",
		"import { OpenAIEmbeddings } from \"@langchain/openai\"",
		"import { MemoryVectorStore } from \"langchain/vectorstores/memory\"",
		"import { createRetrievalChain } from \"langchain/chains/retrieval\"",
		"import { PromptTemplate } from \"@langchain/core/prompts\"",
		"import { StringOutputParser } from \"@langchain/core/output_parsers\"",
		"import * as dotenv from \"dotenv\"",
		"",
		"dotenv.config()",
		"",
		"const model = new ChatOpenAI({})",
		"",
		"const prompt = ChatPromptTemplate.fromTemplate(",
		"    `Answer the user's question from the following context: ",
		"    {context}",
		"    Question: {input}`",
		")",
		"",
		"let retrievalChain, splitDocs",
		"",
		"async function loadDocumentsFromPDF(url) {",
		"    const loader = new PDFLoader(url)",
		"    const docs = await loader.load()",
		"",
		"    const splitter = new RecursiveCharacterTextSplitter({",
		"        chunkSize: 100,",
		"        chunkOverlap: 20,",
		"    })",
		"",
		"    splitDocs = await splitter.splitDocuments(docs)",
		"",
		"    const embeddings = new OpenAIEmbeddings()",
		"",
		"    const vectorstore = await MemoryVectorStore.fromDocuments(",
		"        splitDocs,",
		"        embeddings",
		"    )",
		"    ",
		"    const retriever = vectorstore.asRetriever()",
		"",
		"    const chain = await createStuffDocumentsChain({",
		"        llm: model,",
		"        prompt",
		"    })",
		"",
		"    retrievalChain = await createRetrievalChain({",
		"        combineDocsChain: chain,",
		"        retriever",
		"    })",
		"}",
		"",
		"await loadDocumentsFromPDF(\"daniel.pdf\")",
		"",
		"console.log(\"âœ… document loaded\")",
		"",
		"const {answer} = await retrievalChain.invoke({",
		"    input: \"What is the name of Daniel's cat?\",",
		"    context: splitDocs",
		"})",
		"",
		"console.log(answer)",
		"",
		"const storyPrompt = new PromptTemplate({",
		"    inputVariables: [ \"sentence\"],",
		"    template: \"Tell me a story based on the characters from this sentence: {sentence}\"",
		"})",
		"",
		"const chain = storyPrompt.pipe(model).pipe(new StringOutputParser())",
		"",
		"// let story = await chain.invoke({sentence: answer})",
		"// console.log(story)",
		"// ðŸŸ¢ invoke VS streaming",
		"// ðŸŸ¢ LLMs autocomplete one token at a time; if 2 + 2 = 5 they will try to justify ",
		"const stream = await chain.stream({sentence: answer})",
		"const chunks = [];",
		"for await (const chunk of stream) {",
		"  chunks.push(chunk);",
		"  // console.log(chunk)",
		"  process.stdout.write(chunk)",
		"}"
	  ],
	  "description": "Snippet for streaming a story based on characters in LangChain."
}, 

"d3-start": {
	"prefix": "d3-start",
	"body": [
	"import { END, START, MessageGraph } from \"@langchain/langgraph\"",
	"",
	"const funcA = input => { ",
	"  input[0].content += \"Agent takes action A; \";",
	"  return input ",
	"}",
	"",
	"const funcB = input => { ",
	"  input[0].content += \"Agent takes action B; \";",
	"  return input ",
	"}",
	"",
	"// build the graph",
	"const graph = new MessageGraph()",
	"    // nodes",
	"    .addNode(\"nodeA\", funcA)",
	"    .addNode(\"nodeB\", funcB)",
	"    // edges",
	"    .addEdge(START, \"nodeA\")",
	"    .addEdge(\"nodeA\", \"nodeB\")",
	"    .addEdge(\"nodeB\", END)",
	"",
	"const runnable = graph.compile()",
	"const result = await runnable.invoke('Input; ')",
	"console.log(result)"
	],
	"description": "LangGraph example snippet"
}, 

"d3-conditional-edges": {
	"prefix": "d3-conditional-edges",
	"body": [
	"import { END, START, MessageGraph } from '@langchain/langgraph'",
	"",
	"const funBuy = input => { ",
	"    input[0].content += ' ==> Agent will buy stocks'; ",
	"    return input ",
	"}",
	"",
	"const funSell = input => {",
	"    input[0].content += ' ==> Agent will sell stocks';",
	"    return input ",
	"}",
	"",
	"const funDecision = input => {",
	"    const last = input[0].content",
	"    const isMarketDown = last.includes('SP500') && last.includes('down')",
	"    return isMarketDown ? ",
	"        'actionBuyStocks':",
	"        'actionSellSocks'",
	"}",
	"",
	"const graph = new MessageGraph()",
	"",
	"// setup nodes",
	"graph.addNode('decision', funDecision)",
	"    .addNode('actionBuyStocks', funBuy)",
	"    .addNode('actionSellSocks', funSell)",
	"",
	"// setup edges",
	"graph.addEdge(START, 'decision')",
	"    .addConditionalEdges(",
	"        'decision', ",
	"        funDecision, ",
	"        ['actionBuyStocks', 'actionSellSocks']",
	"    )",
	"    .addEdge('actionBuyStocks', END)",
	"    .addEdge('actionSellSocks', END)",
	"",
	"const runnable = graph.compile()",
	"const result = await runnable.invoke('Latest news SP500 is down to 5000')",
	"console.log(result)"
	],
	"description": "LangGraph conditional edges example snippet"
}, 

"d3-display-graph": {
	"prefix": "d3-display-graph",
	"body": [
	"import { END, START, MessageGraph } from \"@langchain/langgraph\"",
	"import * as fs from \"fs\"",
	"",
	"// define the nodes",
	"const funcA = input => { input[0].content += \"A\"; return input }",
	"const funcB = input => { input[0].content += \"B\"; return input }",
	"const funcC = input => { input[0].content += \"C\"; return input }",
	"const funcD = input => { input[0].content += \"D\"; return input }",
	"const funcE = input => { input[0].content += \"E\"; return input }",
	"",
	"const graph = new MessageGraph()",
	"",
	"// build nodes",
	"graph.addNode(\"nodeA\", funcA)",
	"graph.addNode(\"nodeB\", funcB)",
	"graph.addNode(\"nodeC\", funcC)",
	"graph.addNode(\"nodeD\", funcD)",
	"graph.addNode(\"funcE\", funcE)",
	"",
	"// add node connections using edges",
	"graph.addEdge(START, \"nodeA\")",
	"graph.addEdge(\"nodeA\", \"nodeB\")",
	"graph.addEdge(\"nodeA\", \"nodeC\")",
	"graph.addEdge(\"nodeA\", \"nodeD\")",
	"graph.addEdge(\"nodeB\", \"funcE\")",
	"graph.addEdge(\"nodeC\", \"funcE\")",
	"graph.addEdge(\"nodeD\", \"funcE\")",
	"graph.addEdge(\"funcE\", END)",
	"",
	"// printing the graph as a PNG image",
	"const FILE_NAME = \"graph-struct.png\"",
	"const runnable = graph.compile()",
	"const image = await runnable.getGraph().drawMermaidPng();",
	"const arrayBuffer = await image.arrayBuffer();",
	"await fs.writeFileSync(FILE_NAME, new Uint8Array(arrayBuffer))",
	"console.log(\"Graph structure exported to: \" + FILE_NAME)"
	],
	"description": "LangGraph display graph example snippet"
},

"d3-loop": {
	  "prefix": "d3-loop",
	  "body": [
		"import { END, START, MessageGraph } from '@langchain/langgraph'",
		"import { HumanMessage } from '@langchain/core/messages'",
		"",
		"const funAgent = input => input",
		"const funUseDiceTool = input => {",
		"    // random dice - 1 to 6",
		"    const dice = 1 + Math.floor(Math.random() * 6)",
		"    const content = (dice !== 6) ? ",
		"        'Dice rolled: ' + dice : ",
		"        'objective_achieved'",
		"    input.push(new HumanMessage(content))",
		"    return input",
		"}",
		"",
		"const shouldContinue = input => {",
		"    return input.pop().content.includes('objective_achieved') ? ",
		"        'end' :",
		"        'useDiceTool'",
		"}",
		"    ",
		"const graph = new MessageGraph()",
		"",
		"// setup nodes",
		"graph.addNode('agent', funAgent)",
		"    .addNode('useDiceTool', funUseDiceTool)",
		"",
		"// setup edges",
		"graph.addEdge(START, 'agent')",
		"    .addEdge('useDiceTool', 'agent')",
		"    .addConditionalEdges(",
		"        'agent', ",
		"        shouldContinue, ",
		"        {'useDiceTool': 'useDiceTool', 'end': END}",
		"    )",
		"",
		"const runnable = graph.compile()",
		"const result = await runnable.invoke('Start game')",
		"console.log(result)"
	  ],
	  "description": "LangGraph cycles example snippet"
},

"d3-first-agent": {
	"prefix": "d3-first-agent",
	"body": [
	"import { HumanMessage, SystemMessage } from \"@langchain/core/messages\"",
	"import { ToolNode } from \"@langchain/langgraph/prebuilt\"",
	"import {",
	"  END, MessagesAnnotation, START, StateGraph",
	"} from \"@langchain/langgraph\"",
	"import { ChatOpenAI } from \"@langchain/openai\"",
	"import { tool } from \"@langchain/core/tools\"",
	"import { z } from \"zod\"",
	"import * as dotenv from \"dotenv\"",
	"",
	"dotenv.config()",
	"",
	"const llm = new ChatOpenAI({ model: \"gpt-4o\", temperature: 0 })",
	"",
	"const getLastMessage = ({ messages }) => messages[messages.length - 1]",
	"",
	"const gmtTimeSchema = z.object({",
	"  city: z.string().describe(\"The name of the city\")",
	"})",
	"",
	"const gmtTimeTool = tool(",
	"  async ({ city }) => {",
	"    const serviceIsWorking = Math.floor(Math.random() * 3)",
	"    return serviceIsWorking !== 2",
	"      ? \"The local in \" + city + \" time is 2:30am.\"",
	"      : \"Error 404\"",
	"  },",
	"  {",
	"    name: \"gmtTime\",",
	"    description: `Check local time in a specified city. ",
	"    The API is randomly available every third call.`,",
	"    schema: gmtTimeSchema,",
	"  }",
	")",
	"",
	"const tools = [gmtTimeTool]",
	"const toolNode = new ToolNode(tools)",
	"const llmWithTools = llm.bindTools(tools)",
	"",
	"const callModel = async (state) => {",
	"  const { messages } = state",
	"  const result = await llmWithTools.invoke(messages)",
	"  return { messages: [result] }",
	"}",
	"",
	"const shouldContinue = (state) => {",
	"  const lastMessage = getLastMessage(state)",
	"  const didAICalledAnyTools = lastMessage._getType() === \"ai\" &&",
	"    lastMessage.tool_calls?.length",
	"  return didAICalledAnyTools ? \"tools\" : END",
	"}",
	"",
	"const graph = new StateGraph(MessagesAnnotation)",
	"  .addNode(\"agent\", callModel)",
	"  .addNode(\"tools\", toolNode)",
	"  .addEdge(START, \"agent\")",
	"  .addEdge(\"tools\", \"agent\")",
	"  .addConditionalEdges(\"agent\", shouldContinue, [\"tools\", END])",
	"",
	"const runnable = graph.compile()",
	"",
	"const result = await runnable.invoke({",
	"  messages: [",
	"    new SystemMessage(",
	"      `You are responsible for answering user questions using tools. ",
	"      These tools sometimes fail, but you keep trying until ",
	"      you get a valid response.`",
	"    ),",
	"    new HumanMessage(",
	"      \"What is the time now in Singapore? I would like to call a friend.\"",
	"    ),",
	"  ]",
	"})",
	"",
	"console.log(result)"
	],
	"description": "LangGraph first agent example snippet"
}
}